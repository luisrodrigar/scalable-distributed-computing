{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Data Analytics Using PySpark\n",
    "## Scalable and Distributed Computing\n",
    "Sara Dovalo del Río, Alejandra Estrada Sanz and Luis Ángel Rodríguez García"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "#### PySpark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CharType' from 'pyspark.sql.types' (/opt/homebrew/lib/python3.9/site-packages/pyspark/sql/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/luisrodrigar/Documents/Statistics for Data Science/Scaled and Distributed Computation/Project/scalable-distributed-computing/Lab 2 - PySpark/pyspark-parallel.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/pyspark-parallel.ipynb#ch0000004?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msession\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/pyspark-parallel.ipynb#ch0000004?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType, StructField\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/pyspark-parallel.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m LongType, TimestampType, IntegerType, DoubleType, StringType, CharType\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/pyspark-parallel.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m when, count, col, countDistinct, desc, first, lit\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/pyspark-parallel.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display, Markdown\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CharType' from 'pyspark.sql.types' (/opt/homebrew/lib/python3.9/site-packages/pyspark/sql/types.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import LongType, TimestampType, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.functions import when, count, col, countDistinct, desc, first, lit\n",
    "from IPython.display import display, Markdown\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "seed_value = 100469000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- rateCodeId: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improve_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This DataFrame has **1942420 rows**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_schema = StructType([StructField('VendorID', LongType(), False),\n",
    "                     StructField('tpep_pickup_datetime', TimestampType(), False),\n",
    "                     StructField('tpep_dropoff_datetime', TimestampType(), False),\n",
    "                     StructField('passenger_count', IntegerType(), False),\n",
    "                     StructField('trip_distance', DoubleType(), False),\n",
    "                     StructField('rateCodeId', IntegerType(), False),\n",
    "                     StructField('store_and_fwd_flag', StringType(), False),\n",
    "                     StructField('PULocationID', StringType(), False),\n",
    "                     StructField('DOLocationID', StringType(), False),         \n",
    "                     StructField('payment_type', IntegerType(), False),\n",
    "                     StructField('fare_amount', DoubleType(), False),\n",
    "                     StructField('extra', DoubleType(), False),\n",
    "                     StructField('mta_tax', DoubleType(), False),\n",
    "                     StructField('tip_amount', DoubleType(), False),\n",
    "                     StructField('tolls_amount', DoubleType(), False),\n",
    "                     StructField('improve_surcharge', DoubleType(), False),\n",
    "                     StructField('total_amount', DoubleType(), False)])\n",
    "\n",
    "cab_trips = spark.read \\\n",
    "                .schema(custom_schema) \\\n",
    "                .option('header', 'true') \\\n",
    "                .csv('data/*.csv')\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "cab_trips.printSchema()\n",
    "display(Markdown(\"This DataFrame has **%d rows**.\" % (cab_trips.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the method `cache` to do the optimization of the dataset in order to do the processing faster. Later on, we are going to generate a random sample of the ten percent of the rows without replacemant and select the two first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 01:36:40 WARN CacheManager: Asked to cache already cached data.\n",
      "22/03/17 01:36:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount\n",
      " Schema: VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, rateCodeId, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improve_surcharge, total_amount\n",
      "Expected: improve_surcharge but found: improvement_surcharge\n",
      "CSV file: file:///Users/luisrodrigar/Documents/Statistics%20for%20Data%20Science/Scaled%20and%20Distributed%20Computation/Project/scalable-distributed-computing/Lab%202%20-%20PySpark/data/tripdata_2017-01.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2017, 1, 1, 0, 0, 2), tpep_dropoff_datetime=datetime.datetime(2017, 1, 1, 0, 39, 22), passenger_count=4, trip_distance=7.75, rateCodeId=1, store_and_fwd_flag=None, PULocationID='186', DOLocationID='36', payment_type=1, fare_amount=22.0, extra=0.5, mta_tax=0.5, tip_amount=4.66, tolls_amount=0.0, improve_surcharge=0.3, total_amount=27.96),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2017, 1, 1, 0, 0, 6), tpep_dropoff_datetime=datetime.datetime(2017, 1, 1, 0, 16, 5), passenger_count=2, trip_distance=2.6, rateCodeId=1, store_and_fwd_flag=None, PULocationID='79', DOLocationID='163', payment_type=1, fare_amount=12.5, extra=0.5, mta_tax=0.5, tip_amount=2.76, tolls_amount=0.0, improve_surcharge=0.3, total_amount=16.56)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cab_trips.cache()\n",
    "cab_trips.sample(False, 0.1, seed_value).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of columns incident_id, n_killed, n_injured, n_guns_involved:\")\n",
    "cab_trips.select(\"incident_id\",\"n_killed\",\"n_injured\", \"n_guns_involved\").summary().show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
